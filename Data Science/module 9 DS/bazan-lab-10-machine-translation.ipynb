{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1878727,"sourceType":"datasetVersion","datasetId":1118439}],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1 style=\"font-size:300%; background-color:skyblue; color:grey; padding:60px; font-family:Garamond;\"><b>MACHINE TRANSLATION Using Seq2Seq Modelling</b></h1></center>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:white; color:lightwhite; padding:15px; font-family:Garamond;\">“If you talk to a man in a language he understands, that goes to his head. If you talk to him in his own language, that goes to his heart.”<br> – <b>Nelson Mandela</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<img src='https://miro.medium.com/max/875/1*WWXJ0w6YByfPA9KKmDx2Ug.jpeg' style=\"width:1200px;height:600px;\">","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Intro to Notebook</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">The objective is to Explain How Seq 2 Seq and LSTMs are used for Machine Translations using an example dataset of converting a German sentence to its English counterpart.</p>\n\n<h1> What is Seq2Seq Modelling ?</h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to German). Our aim is to translate given sentences from German language to English.</li>\n    <li style=\"font-size:150%;\">Sequence-to-Sequence (seq2seq) models are used for a variety of NLP tasks, such as text summarization, speech recognition, DNA sequence modeling, among others.</li>\n    <li style=\"font-size:150%;\">Here, both the input and output are sentences. In other words, these sentences are a sequence of words going in and out of a model. This is the basic idea of Sequence-to-Sequence modeling. The figure below tries to explain this method.</li>\n</ul>\n\n<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/01/enc_dec_simple.png\"></center>\n\n<h2>  Here's how it works:</h2>\n\n<ul>\n    <li style=\"font-size:150%;\">Feed the embedding vectors for source sequences (German), to the encoder network, one word at a time.</li>\n    <li style=\"font-size:150%;\">Encode the input sentences into fixed dimension state vectors. At this step, we get the hidden and cell states from the encoder LSTM, and feed it to the decoder LSTM.</li>\n    <li style=\"font-size:150%;\">These states are regarded as initial states by decoder. Additionally, it also has the embedding vectors for target words (English).</li>\n    <li style=\"font-size:150%;\">Decode and output the translated sentence, one word at a time. In this step, the output of the decoder is sent to a softmax layer over the entire target vocabulary.</li>\n</ul>\n\n<h1> What is LSTM ?</h1>\n\n<ul>\n    <li style=\"font-size:150%;\">Long Short-Term Memory (LSTM) networks are a modified version of recurrent neural networks, which makes it easier to remember past data in memory. The vanishing gradient problem of RNN is resolved here. LSTM is well-suited to classify, process and predict time series given time lags of unknown duration. It trains the model by using back-propagation. In an LSTM network, three gates are present:</li>\n</ul>\n\n<center><img src=\"https://miro.medium.com/max/700/1*MwU5yk8f9d6IcLybvGgNxA.jpeg\"></center>\n\n<ul>\n    <li style=\"font-size:150%;\"><b>Input gate —</b> discover which value from input should be used to modify the memory. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1.</li>\n    <center><img src=\"https://miro.medium.com/max/500/1*k1lxwjsxxn8O4BEiVlQNdg.png\"></center>\n    <li style=\"font-size:150%;\"><b>Forget gate —</b> discover what details to be discarded from the block. It is decided by the sigmoid function. it looks at the previous state(ht-1) and the content input(Xt) and outputs a number between 0(omit this)and 1(keep this)for each number in the cell state Ct−1.</li>\n    <center><img src=\"https://miro.medium.com/max/500/1*bQnecA5sy_eepNkL8I-95A.png\"></center>\n    <li style=\"font-size:150%;\"><b>Output gate —</b> the input and the memory of the block is used to decide the output. Sigmoid function decides which values to let through 0,1. and tanh function gives weightage to the values which are passed deciding their level of importance ranging from-1 to 1 and multiplied with output of Sigmoid.</li>\n    <center><img src=\"https://miro.medium.com/max/700/1*s8532P11PgGi2sZqikZ2kA.png\"></center>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Let's start the Implementation</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"### 1. Import the Required Libraries","metadata":{}},{"cell_type":"code","source":"import string\nimport re\nfrom numpy import array, argmax, random, take\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 200)\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:02.119219Z","iopub.execute_input":"2025-04-16T01:01:02.119671Z","iopub.status.idle":"2025-04-16T01:01:08.745256Z","shell.execute_reply.started":"2025-04-16T01:01:02.119581Z","shell.execute_reply":"2025-04-16T01:01:08.744181Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 2. Load the Data ","metadata":{}},{"cell_type":"code","source":"# defining a function to read raw text file\ndef read_text(filename):\n        # open the file\n        file = open(filename, mode='rt', encoding='utf-8')\n        \n        # read all text\n        text = file.read()\n        file.close()\n        return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:08.753890Z","iopub.execute_input":"2025-04-16T01:01:08.754259Z","iopub.status.idle":"2025-04-16T01:01:08.768816Z","shell.execute_reply.started":"2025-04-16T01:01:08.754222Z","shell.execute_reply":"2025-04-16T01:01:08.767638Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# defining a function to split a text into sentences\ndef to_lines(text):\n    sents = text.strip().split('\\n')\n    sents = [i.split('\\t') for i in sents]\n    return sents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:14.890207Z","iopub.execute_input":"2025-04-16T01:01:14.890584Z","iopub.status.idle":"2025-04-16T01:01:14.895250Z","shell.execute_reply.started":"2025-04-16T01:01:14.890542Z","shell.execute_reply":"2025-04-16T01:01:14.894279Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Loading the dataset (deu.txt) from the Kaggle site\n\ndata = read_text(\"../input/bilingual-sentence-pairs/deu.txt\")\ndeu_eng = to_lines(data)\ndeu_eng = array(deu_eng)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:16.307592Z","iopub.execute_input":"2025-04-16T01:01:16.307966Z","iopub.status.idle":"2025-04-16T01:01:19.145105Z","shell.execute_reply.started":"2025-04-16T01:01:16.307929Z","shell.execute_reply":"2025-04-16T01:01:19.143848Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Let's get a sample of the dataset (50000/150000)\ndeu_eng = deu_eng[:50000,:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:19.146483Z","iopub.execute_input":"2025-04-16T01:01:19.146805Z","iopub.status.idle":"2025-04-16T01:01:19.150440Z","shell.execute_reply.started":"2025-04-16T01:01:19.146769Z","shell.execute_reply":"2025-04-16T01:01:19.149460Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">The actual data contains over 150,000 sentence-pairs. However, we will use only the first 50,000 sentence pairs to reduce the training time of the model. You can change this number as per your system’s computation power.</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"### 3. Text Cleaning / Preprocessing","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We will get rid of the punctuation marks and then convert all the text to lower case.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# Remove punctuation\n\ndeu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\ndeu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n\n# Displaying the 50000 sentence-pairs\ndeu_eng","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:01:58.946631Z","iopub.execute_input":"2025-04-16T01:01:58.946990Z","iopub.status.idle":"2025-04-16T01:01:59.499921Z","shell.execute_reply.started":"2025-04-16T01:01:58.946957Z","shell.execute_reply":"2025-04-16T01:01:59.498913Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([['Go', 'Geh',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)'],\n       ['Hi', 'Hallo',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)'],\n       ['Hi', 'Grüß Gott',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)'],\n       ...,\n       ['Im giving up smoking', 'Ich höre mit dem Rauchen auf',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #256952 (minshirui) & #407184 (MUIRIEL)'],\n       ['Im glad I was nearby', 'Ich bin froh dass ich in der Nähe war',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #2547219 (CK) & #3448316 (Pfirsichbaeumchen)'],\n       ['Im glad Tom has gone', 'Ich bin froh dass Tom weg ist',\n        'CC-BY 2.0 (France) Attribution: tatoeba.org #2547217 (CK) & #5299642 (Pfirsichbaeumchen)']],\n      dtype='<U537')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# convert text to lowercase\n\nfor i in range(len(deu_eng)):\n    deu_eng[i,0] = deu_eng[i,0].lower()\n    deu_eng[i,1] = deu_eng[i,1].lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:02:12.867780Z","iopub.execute_input":"2025-04-16T01:02:12.868135Z","iopub.status.idle":"2025-04-16T01:02:13.106558Z","shell.execute_reply.started":"2025-04-16T01:02:12.868105Z","shell.execute_reply":"2025-04-16T01:02:13.105661Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# creating empty lists to store the new text of each language\n# after removing the punctuation and converting the text to lowercase\n\neng_l = []\ndeu_l = []\n\n# Creating a for loop to iterate through english language\n# to populate the lists with sentence lengths\nfor i in deu_eng[:,0]:\n      eng_l.append(len(i.split()))\n# creating a for loop to iterate through german language\n# to populate the lists with sentence lengths\nfor i in deu_eng[:,1]:\n      deu_l.append(len(i.split()))\n# Creating a df (pandas dataframe) out of the dictionary\nlength_df = pd.DataFrame({'eng':eng_l, 'deu':deu_l})\n\n#creating a histogram to see the data distribution\nlength_df.hist(bins = 30)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:10:18.874635Z","iopub.execute_input":"2025-04-16T01:10:18.875007Z","iopub.status.idle":"2025-04-16T01:10:19.563354Z","shell.execute_reply.started":"2025-04-16T01:10:18.874976Z","shell.execute_reply":"2025-04-16T01:10:19.562332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":10},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">A Seq2Seq model requires that we convert both the input and the output sentences into integer sequences of fixed length.</li>\n    <li style=\"font-size:150%;\">Now, vectorize our text data by using Keras’s Tokenizer() class. It will turn our sentences into sequences of integers. We can then pad those sequences with zeros to make all the sequences of the same length.</li>\n    <li style=\"font-size:150%;\">Prepare tokenizers for both the German and English sentences</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"# defining a function to build a tokenizer\ndef tokenization(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:10:30.845989Z","iopub.execute_input":"2025-04-16T01:10:30.846334Z","iopub.status.idle":"2025-04-16T01:10:30.851113Z","shell.execute_reply.started":"2025-04-16T01:10:30.846305Z","shell.execute_reply":"2025-04-16T01:10:30.849957Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# prepare english tokenizer\neng_tokenizer = tokenization(deu_eng[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\neng_length = 8\nprint('English Vocabulary Size: %d' % eng_vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:10:36.314351Z","iopub.execute_input":"2025-04-16T01:10:36.314768Z","iopub.status.idle":"2025-04-16T01:10:36.881726Z","shell.execute_reply.started":"2025-04-16T01:10:36.314730Z","shell.execute_reply":"2025-04-16T01:10:36.880422Z"}},"outputs":[{"name":"stdout","text":"English Vocabulary Size: 6256\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# prepare Deutch tokenizer\ndeu_tokenizer = tokenization(deu_eng[:, 1])\ndeu_vocab_size = len(deu_tokenizer.word_index) + 1\n\ndeu_length = 8\nprint('Deutch Vocabulary Size: %d' % deu_vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:11:16.040837Z","iopub.execute_input":"2025-04-16T01:11:16.041184Z","iopub.status.idle":"2025-04-16T01:11:16.662559Z","shell.execute_reply.started":"2025-04-16T01:11:16.041154Z","shell.execute_reply":"2025-04-16T01:11:16.661501Z"}},"outputs":[{"name":"stdout","text":"Deutch Vocabulary Size: 10329\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We tokenized the data — i.e., converted the text to numerical values. This allows the neural network to perform operations on the input data.</li>\n    <li style=\"font-size:150%;\">When we run the tokenizer, it creates a word index, which is then used to convert each sentence to a vector.</li>","metadata":{}},{"cell_type":"code","source":"# Since sentences can vary greatly in length, padding is used to ensure\n# all sequences in a batch have the same length.\n\n# defomomg a function for encoding the sequences\n\ndef encode_sequences(tokenizer, length, lines):\n    seq = tokenizer.texts_to_sequences(lines)\n   \n    # padding the sequences with 0 values\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:12:28.193573Z","iopub.execute_input":"2025-04-16T01:12:28.193908Z","iopub.status.idle":"2025-04-16T01:12:28.198376Z","shell.execute_reply.started":"2025-04-16T01:12:28.193877Z","shell.execute_reply":"2025-04-16T01:12:28.197398Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">When we feed our sequences of word IDs into the model, each sequence needs to be the same length. To achieve this, padding is added to any sequence that is shorter than the max length (i.e. shorter than the longest sentence).</li>\n</ul>\n\n<center><img src=\"https://miro.medium.com/max/1728/0*6jZTOE0P7_i7N8pn.png\"></center>","metadata":{}},{"cell_type":"markdown","source":"<center><h1 style=\"font-size:200%; background-color:skyblue; color:black; padding:15px; font-family:Garamond;\"><b>Model Building</b></h1></center>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:150%;\">CODE REFERENCE: https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/</p>\n\n<p style=\"font-size:170%;\">First, let’s breakdown the architecture of an RNN at a high level. Referring to the diagram above, there are a few parts of the model we to be aware of:</p>\n\n<ul>\n    <li style=\"font-size:150%;\">Inputs. Input sequences are fed into the model with one word for every time step. Each word is encoded as a unique integer so that it maps to the German dataset vocabulary.</li>\n    <li style=\"font-size:150%;\">Embedding Layers. Embeddings are used to convert each word to a vector. The size of the vector depends on the complexity of the vocabulary.</li>\n    <li style=\"font-size:150%;\">LSTM Layer (Encoder). This is where the context from word vectors in previous time steps is applied to the current word vector.</li>\n    <li style=\"font-size:150%;\">Dense Layers (Decoder). These are typical fully connected layers used to decode the encoded input into the correct translation sequence.</li>\n    <li style=\"font-size:150%;\">he outputs are returned as a sequence of integers or one-hot encoded vectors which can then be mapped to the English dataset vocabulary.</li>\n</ul>\n    \n    \n<center><h1 style=\"font-size:150%;\">Model Architecture</h1></center>\n<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/01/architecture.png\"></center>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split data into train and test set\ntrain, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:12:52.848354Z","iopub.execute_input":"2025-04-16T01:12:52.848728Z","iopub.status.idle":"2025-04-16T01:12:53.866235Z","shell.execute_reply.started":"2025-04-16T01:12:52.848690Z","shell.execute_reply":"2025-04-16T01:12:53.865021Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">It’s time to encode the sentences. We will encode German sentences as the input sequences and English sentences as the target sequences. This has to be done for both the train and test datasets.</li>","metadata":{}},{"cell_type":"code","source":"# prepare training data\ntrainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n\n# prepare validation data\ntestX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:14:07.468338Z","iopub.execute_input":"2025-04-16T01:14:07.468757Z","iopub.status.idle":"2025-04-16T01:14:09.069558Z","shell.execute_reply.started":"2025-04-16T01:14:07.468721Z","shell.execute_reply":"2025-04-16T01:14:09.068583Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 5.Define the Model","metadata":{}},{"cell_type":"code","source":"# build NMT model\ndef define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n    model = Sequential()\n    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n    model.add(LSTM(units))\n    model.add(RepeatVector(out_timesteps))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(Dense(out_vocab, activation='softmax'))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:14:28.249023Z","iopub.execute_input":"2025-04-16T01:14:28.249358Z","iopub.status.idle":"2025-04-16T01:14:28.254956Z","shell.execute_reply.started":"2025-04-16T01:14:28.249329Z","shell.execute_reply":"2025-04-16T01:14:28.253752Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# model compilation\nmodel = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:14:32.696610Z","iopub.execute_input":"2025-04-16T01:14:32.696953Z","iopub.status.idle":"2025-04-16T01:14:34.440496Z","shell.execute_reply.started":"2025-04-16T01:14:32.696922Z","shell.execute_reply":"2025-04-16T01:14:34.439467Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"rms = optimizers.RMSprop(lr=0.001)\nmodel.compile(optimizer=rms, loss='sparse_categorical_crossentropy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:14:36.432902Z","iopub.execute_input":"2025-04-16T01:14:36.433261Z","iopub.status.idle":"2025-04-16T01:14:36.455970Z","shell.execute_reply.started":"2025-04-16T01:14:36.433230Z","shell.execute_reply":"2025-04-16T01:14:36.455000Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">We are using the RMSprop optimizer in this model as it’s usually a good choice when working with recurrent neural networks.</li>\n    <li style=\"font-size:150%;\">Here I have used ‘sparse_categorical_crossentropy‘ as the loss function. This is because the function allows us to use the target sequence as is, instead of the one-hot encoded format. One-hot encoding the target sequences using such a huge vocabulary might consume our system’s entire memory.</li>\n               \n</ul>","metadata":{}},{"cell_type":"markdown","source":"### 6.Fit the Model","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li style=\"font-size:150%;\">ModelCheckpoint() function to save the model with the lowest validation loss. I personally prefer this method over early stopping.</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"filename = 'model.h1.24_jan_19'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\n# train model\nhistory = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:15:06.617602Z","iopub.execute_input":"2025-04-16T01:15:06.617983Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n63/63 [==============================] - 133s 2s/step - loss: 4.3598 - val_loss: 2.8280\n\nEpoch 00001: val_loss improved from inf to 2.82799, saving model to model.h1.24_jan_19\nEpoch 2/30\n63/63 [==============================] - 126s 2s/step - loss: 2.7836 - val_loss: 2.7476\n\nEpoch 00002: val_loss improved from 2.82799 to 2.74755, saving model to model.h1.24_jan_19\nEpoch 3/30\n63/63 [==============================] - 126s 2s/step - loss: 2.6160 - val_loss: 2.5510\n\nEpoch 00003: val_loss improved from 2.74755 to 2.55102, saving model to model.h1.24_jan_19\nEpoch 4/30\n63/63 [==============================] - 126s 2s/step - loss: 2.4146 - val_loss: 2.4091\n\nEpoch 00004: val_loss improved from 2.55102 to 2.40912, saving model to model.h1.24_jan_19\nEpoch 5/30\n63/63 [==============================] - 126s 2s/step - loss: 2.2704 - val_loss: 2.3293\n\nEpoch 00005: val_loss improved from 2.40912 to 2.32927, saving model to model.h1.24_jan_19\nEpoch 6/30\n63/63 [==============================] - 126s 2s/step - loss: 2.1412 - val_loss: 2.2058\n\nEpoch 00006: val_loss improved from 2.32927 to 2.20582, saving model to model.h1.24_jan_19\nEpoch 7/30\n63/63 [==============================] - 126s 2s/step - loss: 2.0126 - val_loss: 2.1263\n\nEpoch 00007: val_loss improved from 2.20582 to 2.12631, saving model to model.h1.24_jan_19\nEpoch 8/30\n63/63 [==============================] - 126s 2s/step - loss: 1.9134 - val_loss: 2.0332\n\nEpoch 00008: val_loss improved from 2.12631 to 2.03317, saving model to model.h1.24_jan_19\nEpoch 9/30\n63/63 [==============================] - 126s 2s/step - loss: 1.7964 - val_loss: 1.9783\n\nEpoch 00009: val_loss improved from 2.03317 to 1.97828, saving model to model.h1.24_jan_19\nEpoch 10/30\n63/63 [==============================] - 126s 2s/step - loss: 1.6971 - val_loss: 1.9279\n\nEpoch 00010: val_loss improved from 1.97828 to 1.92788, saving model to model.h1.24_jan_19\nEpoch 11/30\n63/63 [==============================] - 126s 2s/step - loss: 1.6074 - val_loss: 1.8406\n\nEpoch 00011: val_loss improved from 1.92788 to 1.84062, saving model to model.h1.24_jan_19\nEpoch 12/30\n63/63 [==============================] - 126s 2s/step - loss: 1.5140 - val_loss: 1.7896\n\nEpoch 00012: val_loss improved from 1.84062 to 1.78964, saving model to model.h1.24_jan_19\nEpoch 13/30\n63/63 [==============================] - 126s 2s/step - loss: 1.4190 - val_loss: 1.7337\n\nEpoch 00013: val_loss improved from 1.78964 to 1.73371, saving model to model.h1.24_jan_19\nEpoch 14/30\n63/63 [==============================] - ETA: 0s - loss: 1.3414","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train','validation'])\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 7.Prediction on unseen data","metadata":{}},{"cell_type":"code","source":"model = load_model('model.h1.24_jan_19')\npreds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_text = []\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        t = get_word(i[j], eng_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n                temp.append('')\n            else:\n                temp.append(t)\n        else:\n            if(t == None):\n                temp.append('')\n            else:\n                temp.append(t) \n\n    preds_text.append(' '.join(temp))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print 15 rows randomly\npred_df.head(15)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conclusion: \n1. Machine translation is used to convert text from one language to another.\n2. The lab uses a German-to-English dataset to perform translation.\n3. Sequence-to-sequence models are effective for language translation tasks.\n4. Long Short-Term Memory (LSTM) networks help in processing sequences of words.\n5. The encoder processes input sentences into a context vector.\n6. The decoder uses this context to predict the translated output.\n7. The quality of predictions depends on the size and quality of the dataset.\n8. The model improves with more training examples.\n9. Translations often fail when the model hasn't seen similar examples during training.\n10. Word order and grammar rules are challenging for the model to learn.\n11. The model sometimes generates incorrect or meaningless sentences.\n12. Even with simple data, the model does not always translate correctly.\n13. Human interpretation is still more accurate than machine translation.\n14. Errors are common when translating longer or more complex sentences.\n15. Training takes a long time but helps improve prediction accuracy.\n16. Predictions become more accurate as the model sees more patterns.\n17. Translations are limited by the model’s vocabulary and structure.\n18. Seq2Seq models can be used for many NLP tasks, including translation.\n19. Neural networks learn to map meaning from one language to another.\n20. Machine translation models can be reused for different datasets.\n21. Data preprocessing is important for better model performance.\n22. Training requires multiple epochs to improve accuracy.\n23. The model uses tokenization to understand sentence structure.\n24. Using GPU can help reduce training time in real projects.\n25. This lab shows that AI can help translate languages but still needs improvement.","metadata":{}},{"cell_type":"markdown","source":"### Author's Notes:\n   1. Our Seq2Seq model does a decent job. But there are several instances where it misses out on understanding the key words.\n 2. These are the challenges you will face on a regular basis in NLP. But these aren’t immovable obstacles. \n3. We can mitigate such challenges by using more training data and building a better (or more complex) model.","metadata":{}},{"cell_type":"markdown","source":"##### End of Kaggle's Machine Translation Lab","metadata":{}}]}