{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 06 - Text Representation - Annette Bazan"
      ],
      "metadata": {
        "id": "8038Rr_2yhJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What is text representation?\n",
        "- Text representation is a process that involes converting textual data into numeric values that can be processed and understood by a machine.\n",
        "- There are 3 popular techniques to perform text representation:\n",
        "1. Bags-of-Words (BoW)\n",
        "2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "3. Word Embedding: Word2Vec"
      ],
      "metadata": {
        "id": "FGjpNqRKzCBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bag-of-Words (BoW)\n",
        "* In this technique the frequency of words is captured.\n",
        "* Using the CountVectorizer from scikit-learn library enables us to create bag of words."
      ],
      "metadata": {
        "id": "shbBk8stzlu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. importing libraries\n",
        "\n",
        "# general libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# BoW library\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "HViZOwVczz1Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. getting the text (document)\n",
        "documents = ['This is the first document.',\n",
        "             'This document is the second document',\n",
        "             'And this is the third one.']"
      ],
      "metadata": {
        "id": "5eJ2NHDx0Juy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. let's create the bag-of-words model using unigram, bigram and trigram\n",
        "# n-gram is a sequence of words or characters that are together\n",
        "bow_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "x = bow_vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "pDujFzz40O6i"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. creating the features from the document as the output of the model\n",
        "print(bow_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp8c__3H0SV4",
        "outputId": "a252b033-850c-4731-ba66-d7055cef708d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'and this' 'and this is' 'document' 'document is' 'document is the'\n",
            " 'first' 'first document' 'is' 'is the' 'is the first' 'is the second'\n",
            " 'is the third' 'one' 'second' 'second document' 'the' 'the first'\n",
            " 'the first document' 'the second' 'the second document' 'the third'\n",
            " 'the third one' 'third' 'third one' 'this' 'this document'\n",
            " 'this document is' 'this is' 'this is the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. coverting text (documents) into numeric values\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvp4PTHP0WuK",
        "outputId": "14e58128-ad64-4145-c0eb-7cb8ba603363"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 1 1]\n",
            " [0 0 0 2 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 1 1 1 0 0]\n",
            " [1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion for BoW:**\n",
        "\n",
        "1. **List 5 points you learned about BoW technique**\n",
        "* Import necessary libraries like NumPy, pandas, and visualization tools (Matplotlib, Seaborn).\n",
        "* Use CountVectorizer from sklearn.feature_extraction.text to implement BoW.\n",
        "* Prepare a list of text documents as input data.\n",
        "* Convert text into a bag-of-words model using unigram, bigram, and trigram.\n",
        "* Transform text into numerical feature vectors and display the output.\n",
        "2. **How is BoW performing text representation?**\n",
        "BoW (Bag of Words) represents text by counting word occurrences, ignoring grammar and word order. It creates a sparse vector where each word is a feature with its frequency in the text.\n",
        "3. **What are the limitations of BoW technique?**\n",
        "It ignores context, word meaning, and semantic relationships, leading to high dimensionality and data sparsity."
      ],
      "metadata": {
        "id": "V7J6SV220asu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.**TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "- While BoW perform text represenation without any meaningful information, TF-IDF runs text representation based on the significance and importance of each word in the context.\n",
        "- TF: number of word occurance in the document,\n",
        "- IDF: log base e (total number of documents / number of documents that are having the word, term)\n",
        "- TF-IDF: TF * IDF"
      ],
      "metadata": {
        "id": "4fkF5C0w1qWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. importing libraries\n",
        "\n",
        "# general libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# TF-IDF library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "Zi4f7LfR2CXe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. getting the text (document)\n",
        "documents = ['This is the first document.',\n",
        "             'This document is the second document',\n",
        "             'And this is the third one.']"
      ],
      "metadata": {
        "id": "FFdTzvzu2GH9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. let's create the TF-IDF model\n",
        "# creating TF, IDF, TF-IDF = TF * IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "x_tfidf = tfidf_vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "sci60OU72GQX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. creating the features from the document as the output of the model\n",
        "print(tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZdr5qZd2K9T",
        "outputId": "7eb14b15-0320-4db8-c10b-da68750eaeac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. coverting text (documents) into numeric values\n",
        "# printing out the TF-IDF matrix\n",
        "print(x_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0HvqhFh2LVp",
        "outputId": "710ba37b-470d-4eaf-f8ac-a12a6ba6c60c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
            "  0.3645444  0.         0.3645444 ]\n",
            " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
            "  0.28285122 0.         0.28285122]\n",
            " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
            "  0.29360705 0.49711994 0.29360705]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion for TF-IDF:**\n",
        "\n",
        "1. **List 5 points you learned about TF-IDF technique.**\n",
        "- TfidfVectorizer from sklearn.feature_extraction.text is used to implement the technique.\n",
        "- TF-IDF assigns weights to words based on their importance, reducing the impact of common words.\n",
        "- It calculates Term Frequency (TF), Inverse Document Frequency (IDF), and their product (TF-IDF).\n",
        "- The output is a feature matrix where each word is represented by its TF-IDF score.\n",
        "- It converts text documents into numerical vectors, making them suitable for machine learning models.\n",
        "2. **How is TF-IDF performing text representation?**\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) represents text by weighting words based on their frequency in a document and rarity across documents.\n",
        "It reduces the importance of common words and highlights unique terms for better text representation.\n",
        "3. **What are the limitations of TF-IDF technique?**\n",
        "It ignores word order, context, and semantic meaning, and it may not perform well with short texts or dynamic vocabulary changes.\n"
      ],
      "metadata": {
        "id": "YiNVUDMn2MGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.**Word Embedding (Word2Vec)**\n",
        "- While TF-IDF perform text represenation without any meaningful relationshiop between the words, Word2Vec offers meaningful relationships and contextual and semantic nuances."
      ],
      "metadata": {
        "id": "-Eo1BRtH3Xp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. importing libraries\n",
        "\n",
        "# general libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# word2vec libraries\n",
        "# gensim library: it needs to be installed\n",
        "# import nltk"
      ],
      "metadata": {
        "id": "uWJbOEs13gwG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j4jMpmY3mPK",
        "outputId": "3a0edac7-fd48-4567-ea69-d9073b62f9d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, gensim\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EmrKqRd3pPI",
        "outputId": "c4ff00e8-cca8-449c-e651-3ac42d0d3e31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. getting the text (document)\n",
        "documents = ['This is the first document.',\n",
        "             'This document is the second document',\n",
        "             'And this is the third one.']"
      ],
      "metadata": {
        "id": "Bu3D95C93r5F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. tokenization of the text and display the words\n",
        "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
        "tokenized_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKrOsOr73smU",
        "outputId": "fe3ba20f-f850-42b0-fc41-cdb2d577c084"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'is', 'the', 'first', 'document', '.'],\n",
              " ['this', 'document', 'is', 'the', 'second', 'document'],\n",
              " ['and', 'this', 'is', 'the', 'third', 'one', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. creating and training the Word2Vec model\n",
        "w2vec_model = Word2Vec(sentences=tokenized_documents, vector_size= 100,\n",
        "                       window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "3bcqhXUR3umT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. getting the vectors\n",
        "vector_representation = w2vec_model.wv['document']\n",
        "print(vector_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFEEy5Mr3uxX",
        "outputId": "ae9baac4-7be7-4a56-f598-41ac27309a1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-5.3672609e-04  2.3644192e-04  5.1034638e-03  9.0088481e-03\n",
            " -9.3043381e-03 -7.1172416e-03  6.4599109e-03  8.9737894e-03\n",
            " -5.0164633e-03 -3.7643004e-03  7.3806536e-03 -1.5336993e-03\n",
            " -4.5375801e-03  6.5538706e-03 -4.8608989e-03 -1.8169655e-03\n",
            "  2.8765604e-03  9.9168427e-04 -8.2857795e-03 -9.4498657e-03\n",
            "  7.3113004e-03  5.0703306e-03  6.7592640e-03  7.6371350e-04\n",
            "  6.3505652e-03 -3.4057787e-03 -9.4622851e-04  5.7676826e-03\n",
            " -7.5218501e-03 -3.9357515e-03 -7.5110761e-03 -9.3052583e-04\n",
            "  9.5381113e-03 -7.3201829e-03 -2.3341659e-03 -1.9368482e-03\n",
            "  8.0780918e-03 -5.9310989e-03  4.5382971e-05 -4.7547058e-03\n",
            " -9.6027423e-03  5.0065457e-03 -8.7603368e-03 -4.3926029e-03\n",
            " -3.5016976e-05 -2.9605580e-04 -7.6620397e-03  9.6154800e-03\n",
            "  4.9820296e-03  9.2333099e-03 -8.1580607e-03  4.4947835e-03\n",
            " -4.1369139e-03  8.2554476e-04  8.4977103e-03 -4.4625821e-03\n",
            "  4.5185820e-03 -6.7884498e-03 -3.5496773e-03  9.3978141e-03\n",
            " -1.5769212e-03  3.2110573e-04 -4.1396245e-03 -7.6837917e-03\n",
            " -1.5078725e-03  2.4704614e-03 -8.8828063e-04  5.5339416e-03\n",
            " -2.7441825e-03  2.2590547e-03  5.4562069e-03  8.3472701e-03\n",
            " -1.4531596e-03 -9.2078000e-03  4.3710154e-03  5.7227572e-04\n",
            "  7.4415724e-03 -8.1270043e-04 -2.6386559e-03 -8.7527474e-03\n",
            " -8.5558672e-04  2.8257391e-03  5.4019219e-03  7.0521878e-03\n",
            " -5.7035559e-03  1.8577738e-03  6.0891090e-03 -4.7976170e-03\n",
            " -3.1079031e-03  6.7982422e-03  1.6323173e-03  1.9082324e-04\n",
            "  3.4728362e-03  2.1747321e-04  9.6202325e-03  5.0613903e-03\n",
            " -8.9175971e-03 -7.0430269e-03  9.0171210e-04  6.3924547e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In Word2Vec:\n",
        "1. numeric value is assigned to a word based on its context within the text,\n",
        "2. the machine understand the relationships between the words based on their numeric value,\n",
        "3. translates words to mathematical vectors for the machine to understand the words semantic values."
      ],
      "metadata": {
        "id": "EUDjru203x9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion:**\n",
        "- Word2Vec captures word relationships and contextual meaning, unlike BoW and TF-IDF.\n",
        "- It represents words as dense numerical vectors instead of sparse frequency-based representations.\n",
        "- Word2Vec can identify word similarities using cosine similarity between vectors.\n",
        "- The gensim library provides an efficient implementation of Word2Vec.\n",
        "- Tokenization is necessary before training the Word2Vec model.\n",
        "- Word2Vec has two main architectures: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
        "- CBOW predicts a word based on its surrounding context.\n",
        "- Skip-Gram predicts surrounding words given a target word.\n",
        "- A larger window size captures broader context, while a smaller one focuses on local meaning.\n",
        "- Word2Vec requires sufficient training data for meaningful word representations.\n",
        "- Words with similar meanings have closer vector representations.\n",
        "- It helps in NLP tasks like word clustering and document classification.\n",
        "- Word2Vec does not account for words with multiple meanings.\n",
        "- The quality of word embeddings improves with more training data.\n",
        "- The output vectors can be visualized using techniques like PCA or t-SNE.\n",
        "- Unlike TF-IDF, Word2Vec captures semantic and syntactic word properties.\n",
        "- Pre-trained Word2Vec models (e.g., Google’s) can be used for various NLP applications.\n",
        "- Training a Word2Vec model requires tuning parameters like vector_size, window, and min_count.\n",
        "- Words not present in training data won’t have vector representations.\n",
        "- Word2Vec improves machine understanding of text, enabling more advanced NLP applications."
      ],
      "metadata": {
        "id": "mnR33TQP368O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### End of Lab 06"
      ],
      "metadata": {
        "id": "hhCJ62sK38Nf"
      }
    }
  ]
}